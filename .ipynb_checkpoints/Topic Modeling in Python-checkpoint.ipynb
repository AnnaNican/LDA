{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling in Python\n",
    "\n",
    "Topic Modelling is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an unsupervised approach used for finding and observing the bunch of words (called “topics”) in large clusters of texts. There are many approaches for obtaining topics from a text such as – Term Frequency and Inverse Document Frequency. NonNegative Matrix Factorization techniques. Latent Dirichlet Allocation is the most popular topic modeling technique that will be used here.\n",
    "\n",
    "This walkthrough uses the following Python packages:\n",
    "(For Mac/Unix with pip)\n",
    "\n",
    ">*NLTK, a natural language toolkit for Python* A useful package for any natural language processing.\n",
    "```$ sudo pip install -U nltk```\n",
    "> You need to start the NLTK Downloader and download all the data you need.\n",
    "> Open a Python console and do the following:\n",
    ">```import nltk```\n",
    ">```nltk.download()```\n",
    "In the GUI window that opens simply press the 'Download' button to download all corpora or go to the 'Corpora' tab and only download the ones you need/want.\n",
    "\n",
    "\n",
    ">*stop_words* a Python package containing stop words.\n",
    "```$ sudo pip install stop-words.```\n",
    "\n",
    ">*gensim* a topic modeling package containing our LDA model.\n",
    "```$ sudo pip install gensim.```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Start with some text:\n",
    "\n",
    "Paste any text that you would like to analyze\n",
    "\n",
    "**make sure you take care of unicode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARTHUR GREGG SULZBERGER does not remember the first time he visited the family business. \\nHe was young, he says, no older than 6, when he shuffled through the brass-plated revolving doors of \\nthe old concrete hulk on 43rd Street and boarded the elevator up to his fathers and grandfathers offices. \\nHe often visited for a few minutes before taking a trip to the newsroom on the third floor, \\nall typewriters and moldering stacks of paper, and then he had sometimes go down to the \\nsubbasement to take in the oily scents and clanking sounds of the printing press']\n"
     ]
    }
   ],
   "source": [
    "text_blob = ['''ARTHUR GREGG SULZBERGER does not remember the first time he visited the family business. \n",
    "He was young, he says, no older than 6, when he shuffled through the brass-plated revolving doors of \n",
    "the old concrete hulk on 43rd Street and boarded the elevator up to his fathers and grandfathers offices. \n",
    "He often visited for a few minutes before taking a trip to the newsroom on the third floor, \n",
    "all typewriters and moldering stacks of paper, and then he had sometimes go down to the \n",
    "subbasement to take in the oily scents and clanking sounds of the printing press''']\n",
    "print(text_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Your Data\n",
    "    \n",
    "Cleaning is an important step before any text mining task, in this step, we will remove the punctuations, stopwords and normalize the corpus.\n",
    "\n",
    ">**remove stop words**: Certain parts of English speech, like conjunctions (“for”, “or”) or the word “the” are meaningless to a topic model. These terms are called stop words and need to be removed from our token list.\n",
    "The definition of a stop word is flexible and the kind of documents may alter that definition. For example, if we’re topic modeling a collection of music reviews, then terms like “The Who” will have trouble being surfaced because “the” is a common stop word and is usually removed. You can always construct your own stop word list or seek out another package to fit your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "text_blob_clean = [clean(doc).split() for doc in text_blob]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'arthur', u'gregg', u'sulzberger', u'remember', u'first', u'time', u'visited', u'family', u'business', u'young', u'say', u'older', u'6', u'shuffled', u'brassplated', u'revolving', u'door', u'old', u'concrete', u'hulk', u'43rd', u'street', u'boarded', u'elevator', u'father', u'grandfather', u'office', u'often', u'visited', u'minute', u'taking', u'trip', u'newsroom', u'third', u'floor', u'typewriter', u'moldering', u'stack', u'paper', u'sometimes', u'go', u'subbasement', u'take', u'oily', u'scent', u'clanking', u'sound', u'printing', u'press']]\n"
     ]
    }
   ],
   "source": [
    "print text_blob_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Construct Document Term Matrix\n",
    "\n",
    "All the text documents combined is known as the corpus. To run any mathematical model on text corpus, it is a good practice to convert it into a matrix representation. LDA model looks for repeating term patterns in the entire DT matrix. Python provides many great libraries for text mining practices\n",
    "\n",
    ">“gensim” is one such clean and beautiful library to handle text data. It is scalable, robust and efficient. Following code shows how to convert a corpus into a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean) index. \n",
    "dictionary = corpora.Dictionary(text_blob_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_blob_clean]\n",
    "\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply LDA Model\n",
    "\n",
    "\n",
    "Alpha and Beta Hyperparameters – alpha represents document-topic density and Beta represents topic-word density. Higher the value of alpha, documents are composed of more topics and lower the value of alpha, documents contain fewer topics. On the other hand, higher the beta, topics are composed of a large number of words in the corpus, and with the lower value of beta, they are composed of few words.\n",
    "\n",
    ">Number of Topics – Number of topics to be extracted from the corpus. Researchers have developed approaches to obtain an optimal number of topics by using Kullback Leibler Divergence Score. I will not discuss this in detail, as it is too mathematical. For understanding, one can refer to this[1] original paper on the use of KL divergence.\n",
    "\n",
    ">Number of Topic Terms – Number of terms composed in a single topic. It is generally decided according to the requirement. If the problem statement talks about extracting themes or concepts, it is recommended to choose a higher number, if problem statement talks about extracting features or terms, a low number is recommended.\n",
    "\n",
    ">Number of Iterations / passes – Maximum number of iterations allowed to LDA algorithm for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Interpreting the Results:\n",
    "\n",
    "Our LDA model is now stored as ldamodel. \n",
    "\n",
    "We can review our topics with the print_topic and print_topics methods.\n",
    "Each line is a topic with individual topic terms and weights.\n",
    "\n",
    "What does this mean? Each generated topic is separated by a comma. Within each topic are the three most probable words to appear in that topic. Even though our document set is small the model is reasonable. Some things to think about: - health, brocolli and good make sense together. - The second topic is confusing. If we revisit the original documents, we see that drive has multiple meanings: driving a car and driving oneself to improve. This is something to note in our results. - The third topic includes mother and brother, which is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.021*\"time\" + 0.021*\"concrete\" + 0.021*\"street\"'), (1, u'0.036*\"visited\" + 0.021*\"office\" + 0.021*\"trip\"'), (2, u'0.021*\"concrete\" + 0.021*\"shuffled\" + 0.021*\"grandfather\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "This explanation is a little lengthy, but useful for understanding the model we worked so hard to generate.\n",
    "\n",
    "LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution, like the ones in our walkthrough model. In other words, LDA assumes a document is made from the following steps:\n",
    "\n",
    "Determine the number of words in a document. Let’s say our document has 6 words.\n",
    "* Determine the mixture of topics in that document. For example, the document might contain 1/2 the topic “health” and 1/2 the topic “vegetables.”\n",
    "* Using each topic’s multinomial distribution, output words to fill the document’s word slots. In our example, the “health” topic is 1/2 our document, or 3 words. The “health” topic might have the word “diet” at 20% probability or “exercise” at 15%, so it will fill the document word slots based on those probabilities.\n",
    "* Given this assumption of how documents are created, LDA backtracks and tries to figure out what topics would create those documents in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
